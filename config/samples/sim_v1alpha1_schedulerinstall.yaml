apiVersion: sim.llm-d.io/v1alpha1
kind: SchedulerInstall
metadata:
  name: llm-sched-install
  namespace: llm-d-inference-scheduler
spec:
  schedulerNamespace: llm-d-inference-scheduler
  simulatorNamespace: llm-d-sim

  epp:
    enabled: true
    image: ghcr.io/llm-d/llm-d-inference-scheduler:v0.4.0
    replicas: 1
    port: 9002
    poolName: gaie-inference-scheduling
    poolNamespace: llm-d-sim

  gateway:
    enabled: true
    name: infra-inference-scheduling-inference-gateway
    className: istio
    listenerPort: 80
    listenerProtocol: HTTP

  envoyFilter:
    enabled: true

  proxyService:
    name: gaie-inference-scheduling-proxy
    port: 8200
    targetPort: 8200
    selector:
      llm-d.ai/role: decode
      llm-d.ai/inferenceServing: "true"

  routing:
    enabled: true
    httpRouteName: llm-d-inference-scheduling
    backendType: InferencePool
    inferencePool:
      name: gaie-inference-scheduling
      namespace: llm-d-sim
      port: 8200
    parentGateway:
      name: infra-inference-scheduling-inference-gateway
      namespace: llm-d-inference-scheduler

  destinationRule:
    enabled: true
    algorithm: ROUND_ROBIN
    connectionPool:
      http1MaxPendingRequests: 1
      maxRequestsPerConnection: 1
