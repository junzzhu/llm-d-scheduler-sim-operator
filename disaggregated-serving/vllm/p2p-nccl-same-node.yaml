---
apiVersion: v1
kind: Pod
metadata:
  name: vllm-prefill-p2p
  namespace: llm-d-vllm
  labels:
    app: vllm-p2p-test
    role: prefill
    case: p2p-same
spec:
  nodeSelector:
    nvidia.com/gpu.product: "NVIDIA-H100-NVL"
  volumes:
  - name: nfs-volume
    nfs:
      server: 10.17.37.59
      path: /data/tests
      readOnly: false
  containers:
  - name: vllm-prefill
    image: vllm/vllm-openai:latest
    command: ["vllm"]
    args: 
      - "serve"
      - "Qwen/Qwen3-Coder-30B-A3B-Instruct"
      - "--tensor-parallel-size=1"
      - "--port=8000"
      - "--enable-auto-tool-choice"
      - "--tool-call-parser=qwen3_coder"
      - "--kv-cache-dtype=fp8"
      - "--gpu-memory-utilization=0.90"
      - "--max-model-len=2048"
      - '--kv-transfer-config={"kv_connector":"P2pNcclConnector","kv_role":"kv_producer","kv_rank":0,"kv_parallel_size":2,"kv_ip":"0.0.0.0","kv_port":14579}'
    env:
    - name: HF_HOME
      value: /models
    - name: TRANSFORMERS_CACHE
      value: /models
    - name: HF_HUB_ENABLE_HF_TRANSFER
      value: "1"
    - name: NCCL_DEBUG
      value: "INFO"
    - name: NCCL_DEBUG_SUBSYS
      value: "ALL"
    - name: VLLM_LOGGING_LEVEL
      value: "DEBUG"
    ports:
    - containerPort: 8000
      name: http
    - containerPort: 14579
      name: kv-transfer
    readinessProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 10
      periodSeconds: 5
      timeoutSeconds: 2
      failureThreshold: 12
    startupProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 2
      failureThreshold: 90
    resources:
      limits:
        nvidia.com/gpu: 1
    volumeMounts:
    - name: nfs-volume
      mountPath: /models

---
apiVersion: v1
kind: Pod
metadata:
  name: vllm-decode-p2p
  namespace: llm-d-vllm
  labels:
    app: vllm-p2p-test
    role: decode
    case: p2p-same
spec:
  nodeSelector:
    nvidia.com/gpu.product: "NVIDIA-H100-NVL"
  volumes:
  - name: nfs-volume
    nfs:
      server: 10.17.37.59
      path: /data/tests
      readOnly: false
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: vllm-p2p-test
            role: prefill
            case: p2p-same
        topologyKey: kubernetes.io/hostname
  containers:
  - name: vllm-decode
    image: vllm/vllm-openai:latest
    command: ["vllm"]
    args:
      - "serve"
      - "Qwen/Qwen3-Coder-30B-A3B-Instruct"
      - "--tensor-parallel-size=1"
      - "--port=8000"
      - "--enable-auto-tool-choice"
      - "--tool-call-parser=qwen3_coder"
      - "--kv-cache-dtype=fp8"
      - "--gpu-memory-utilization=0.90"
      - "--max-model-len=2048"
      - '--kv-transfer-config={"kv_connector":"P2pNcclConnector","kv_role":"kv_consumer","kv_rank":1,"kv_parallel_size":2,"kv_ip":"0.0.0.0","kv_port":14579}'
    env:
    - name: HF_HOME
      value: /models
    - name: TRANSFORMERS_CACHE
      value: /models
    - name: HF_HUB_ENABLE_HF_TRANSFER
      value: "1"
    - name: NCCL_DEBUG
      value: "INFO"
    - name: NCCL_DEBUG_SUBSYS
      value: "ALL"
    - name: VLLM_LOGGING_LEVEL
      value: "DEBUG"
    ports:
    - containerPort: 8000
      name: http
    - containerPort: 14579
      name: kv-transfer
    readinessProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 10
      periodSeconds: 5
      timeoutSeconds: 2
      failureThreshold: 12
    startupProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 2
      failureThreshold: 90
    resources:
      limits:
        nvidia.com/gpu: 1
    volumeMounts:
    - name: nfs-volume
      mountPath: /models

---
apiVersion: v1
kind: Service
metadata:
  name: vllm-prefill-p2p
  namespace: llm-d-vllm
spec:
  selector:
    app: vllm-p2p-test
    role: prefill
    case: p2p-same
  ports:
  - port: 8000
    targetPort: 8000
    name: http
  - port: 14579
    targetPort: 14579
    name: kv-transfer

---
apiVersion: v1
kind: Service
metadata:
  name: vllm-decode-p2p
  namespace: llm-d-vllm
spec:
  selector:
    app: vllm-p2p-test
    role: decode
    case: p2p-same
  ports:
  - port: 8000
    targetPort: 8000
    name: http
  - port: 14579
    targetPort: 14579
    name: kv-transfer

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-p2p-proxy
  namespace: llm-d-vllm
  labels:
    app: vllm-p2p-test
    role: proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-p2p-test
      role: proxy
  template:
    metadata:
      labels:
        app: vllm-p2p-test
        role: proxy
    spec:
      restartPolicy: Always
      containers:
      - name: proxy
        image: registry.access.redhat.com/ubi9/python-311:latest
        command: ["/bin/bash", "-c"]
        args:
          - |
            pip install --no-cache-dir quart aiohttp && \
            python /app/proxy.py
        env:
        - name: PREFILL_URL
          value: "http://vllm-prefill-p2p:8000"
        - name: DECODE_URL
          value: "http://vllm-decode-p2p:8000"
        - name: PREFILL_ZMQ
          value: "vllm-prefill-p2p:14579"
        - name: DECODE_ZMQ
          value: "vllm-decode-p2p:14579"
        ports:
        - containerPort: 8080
          name: http
        volumeMounts:
        - name: proxy-script
          mountPath: /app
      volumes:
      - name: proxy-script
        configMap:
          name: p2p-proxy-script

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: p2p-proxy-script
  namespace: llm-d-vllm
data:
  proxy.py: |
    #!/usr/bin/env python3
    import os
    import uuid
    import aiohttp
    from quart import Quart, request, make_response

    app = Quart(__name__)

    PREFILL_URL = os.getenv("PREFILL_URL", "http://vllm-prefill-p2p:8000")
    DECODE_URL = os.getenv("DECODE_URL", "http://vllm-decode-p2p:8000")
    PREFILL_ZMQ = os.getenv("PREFILL_ZMQ", "vllm-prefill-p2p:14579")
    DECODE_ZMQ = os.getenv("DECODE_ZMQ", "vllm-decode-p2p:14579")

    AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)

    def random_uuid():
        return str(uuid.uuid4().hex)

    async def forward_request(url, data, request_id):
        async with aiohttp.ClientSession(timeout=AIOHTTP_TIMEOUT) as session:
            headers = {"X-Request-Id": request_id, "Content-Type": "application/json"}
            async with session.post(url=url, json=data, headers=headers) as response:
                if response.status == 200:
                    async for chunk_bytes in response.content.iter_chunked(1024):
                        yield chunk_bytes
                else:
                    error_text = await response.text()
                    raise Exception(f"Request failed: {response.status} - {error_text}")

    @app.route("/v1/completions", methods=["POST"])
    @app.route("/v1/chat/completions", methods=["POST"])
    async def handle_request():
        try:
            original_request_data = await request.get_json()
            prefill_request = original_request_data.copy()
            prefill_request["max_tokens"] = 1
            if "max_completion_tokens" in prefill_request:
                prefill_request["max_completion_tokens"] = 1
            
            request_id = f"___prefill_addr_{PREFILL_ZMQ}___decode_addr_{DECODE_ZMQ}_{random_uuid()}"
            
            print(f"Routing request {request_id}")
            
            async for _ in forward_request(f"{PREFILL_URL}{request.path}", prefill_request, request_id):
                continue
            
            print(f"Prefill complete, forwarding to decode...")
            
            generator = forward_request(f"{DECODE_URL}{request.path}", original_request_data, request_id)
            response = await make_response(generator)
            response.timeout = None
            return response
        except Exception as e:
            import traceback
            print(f"Error in proxy: {e}")
            traceback.print_exc()
            return {"error": str(e)}, 500

    @app.route("/health", methods=["GET"])
    async def health():
        return {"status": "healthy", "prefill": PREFILL_URL, "decode": DECODE_URL}

    if __name__ == "__main__":
        print("Starting P2P NCCL Proxy Server")
        print(f"Prefill: {PREFILL_URL} (ZMQ: {PREFILL_ZMQ})")
        print(f"Decode:  {DECODE_URL} (ZMQ: {DECODE_ZMQ})")
        app.run(host="0.0.0.0", port=8080)

---
apiVersion: v1
kind: Service
metadata:
  name: vllm-p2p-proxy
  namespace: llm-d-vllm
spec:
  selector:
    app: vllm-p2p-test
    role: proxy
  ports:
  - port: 8080
    targetPort: 8080
    name: http
